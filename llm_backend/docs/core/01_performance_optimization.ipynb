{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;核心优化的点：响应速度、吞吐量和资源利用率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. API本身的性能优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型推理的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **GPU 资源优化**：通过修改 `/etc/systemd/system/ollama.service` 文件，添加 `Environment=\"CUDA_VISIBLE_DEVICES=0,1\"` 来指定 `GPU`，`Ollama` 可以自动分配任务，充分利用硬件资源。\n",
    "<br>\n",
    "<br>\n",
    "2. **内存与显存管理**:\n",
    "- 在`API`调用时设置 `keep_alive` 参数，控制大模型在显存中的存活时间，避免频繁加载和卸载模型；\n",
    "- `Ollama` 支持动态调整显存占用，通过修改 `/etc/systemd/system/ollama.service` 文件，添加 `Environment=\"OLLAMA_MAX_LOADED_MODELS=3\"` 来限制同时加载的模型数量，避免显存溢出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 并发和异步处理\n",
    "\n",
    "\n",
    "REST API如果是同步的，每个请求都要等待模型推理完成，这可能导致阻塞。改成异步处理，比如使用队列系统，让客户端轮询结果，或者使用WebSocket，可能会提高并发能力。另外，调整Web服务器的worker数量，比如Gunicorn的worker数，或者Uvicorn的异步 workers，可能提升处理能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 缓存和预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一些重复的请求，如果结果不变，可以缓存响应，减少模型的计算次数。比如使用Redis或Memcached来存储常见问题的回答。另外，CDN缓存静态资源，或者API网关的缓存策略，也能减少后端压力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. 代码优化"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
